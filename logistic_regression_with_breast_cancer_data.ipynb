{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "logistic-regression-with-breast-cancer-data.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "coDnCjvv4-t_"
      },
      "source": [
        "# Colab library to upload files to notebook\n",
        "from google.colab import files\n",
        "\n",
        "# Install Kaggle library\n",
        "!pip install -q kaggle\n",
        "\n",
        "# Upload the API Token file\n",
        "uploaded = files.upload( )\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXs5P5tL6fab",
        "outputId": "63dad603-50f0-4973-9372-095d929766ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "!cp /content/kaggle.json /root/.kaggle/\n",
        "%cd /content/\n",
        "!kaggle datasets download -d uciml/breast-cancer-wisconsin-data\n",
        "!unzip breast-cancer-wisconsin-data.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "breast-cancer-wisconsin-data.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  breast-cancer-wisconsin-data.zip\n",
            "  inflating: data.csv                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-0hGaBrF48Ue",
        "outputId": "74f509da-eae4-4f86-ea35-8403f86f7c55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        }
      },
      "source": [
        "# Reading the Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Reading the Data\n",
        "data = pd.read_csv('/content/data.csv')\n",
        "data.head(20)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.049040</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.015870</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.38</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.1622</td>\n",
              "      <td>0.6656</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.26540</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.013080</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.013400</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.18600</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.040060</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.020580</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.24300</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.074580</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.018670</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.25750</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.024610</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.018850</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.16250</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>843786</td>\n",
              "      <td>M</td>\n",
              "      <td>12.45</td>\n",
              "      <td>15.70</td>\n",
              "      <td>82.57</td>\n",
              "      <td>477.1</td>\n",
              "      <td>0.12780</td>\n",
              "      <td>0.17000</td>\n",
              "      <td>0.15780</td>\n",
              "      <td>0.08089</td>\n",
              "      <td>0.2087</td>\n",
              "      <td>0.07613</td>\n",
              "      <td>0.3345</td>\n",
              "      <td>0.8902</td>\n",
              "      <td>2.217</td>\n",
              "      <td>27.19</td>\n",
              "      <td>0.007510</td>\n",
              "      <td>0.033450</td>\n",
              "      <td>0.03672</td>\n",
              "      <td>0.011370</td>\n",
              "      <td>0.02165</td>\n",
              "      <td>0.005082</td>\n",
              "      <td>15.47</td>\n",
              "      <td>23.75</td>\n",
              "      <td>103.40</td>\n",
              "      <td>741.6</td>\n",
              "      <td>0.1791</td>\n",
              "      <td>0.5249</td>\n",
              "      <td>0.5355</td>\n",
              "      <td>0.17410</td>\n",
              "      <td>0.3985</td>\n",
              "      <td>0.12440</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>844359</td>\n",
              "      <td>M</td>\n",
              "      <td>18.25</td>\n",
              "      <td>19.98</td>\n",
              "      <td>119.60</td>\n",
              "      <td>1040.0</td>\n",
              "      <td>0.09463</td>\n",
              "      <td>0.10900</td>\n",
              "      <td>0.11270</td>\n",
              "      <td>0.07400</td>\n",
              "      <td>0.1794</td>\n",
              "      <td>0.05742</td>\n",
              "      <td>0.4467</td>\n",
              "      <td>0.7732</td>\n",
              "      <td>3.180</td>\n",
              "      <td>53.91</td>\n",
              "      <td>0.004314</td>\n",
              "      <td>0.013820</td>\n",
              "      <td>0.02254</td>\n",
              "      <td>0.010390</td>\n",
              "      <td>0.01369</td>\n",
              "      <td>0.002179</td>\n",
              "      <td>22.88</td>\n",
              "      <td>27.66</td>\n",
              "      <td>153.20</td>\n",
              "      <td>1606.0</td>\n",
              "      <td>0.1442</td>\n",
              "      <td>0.2576</td>\n",
              "      <td>0.3784</td>\n",
              "      <td>0.19320</td>\n",
              "      <td>0.3063</td>\n",
              "      <td>0.08368</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>84458202</td>\n",
              "      <td>M</td>\n",
              "      <td>13.71</td>\n",
              "      <td>20.83</td>\n",
              "      <td>90.20</td>\n",
              "      <td>577.9</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>0.16450</td>\n",
              "      <td>0.09366</td>\n",
              "      <td>0.05985</td>\n",
              "      <td>0.2196</td>\n",
              "      <td>0.07451</td>\n",
              "      <td>0.5835</td>\n",
              "      <td>1.3770</td>\n",
              "      <td>3.856</td>\n",
              "      <td>50.96</td>\n",
              "      <td>0.008805</td>\n",
              "      <td>0.030290</td>\n",
              "      <td>0.02488</td>\n",
              "      <td>0.014480</td>\n",
              "      <td>0.01486</td>\n",
              "      <td>0.005412</td>\n",
              "      <td>17.06</td>\n",
              "      <td>28.14</td>\n",
              "      <td>110.60</td>\n",
              "      <td>897.0</td>\n",
              "      <td>0.1654</td>\n",
              "      <td>0.3682</td>\n",
              "      <td>0.2678</td>\n",
              "      <td>0.15560</td>\n",
              "      <td>0.3196</td>\n",
              "      <td>0.11510</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>844981</td>\n",
              "      <td>M</td>\n",
              "      <td>13.00</td>\n",
              "      <td>21.82</td>\n",
              "      <td>87.50</td>\n",
              "      <td>519.8</td>\n",
              "      <td>0.12730</td>\n",
              "      <td>0.19320</td>\n",
              "      <td>0.18590</td>\n",
              "      <td>0.09353</td>\n",
              "      <td>0.2350</td>\n",
              "      <td>0.07389</td>\n",
              "      <td>0.3063</td>\n",
              "      <td>1.0020</td>\n",
              "      <td>2.406</td>\n",
              "      <td>24.32</td>\n",
              "      <td>0.005731</td>\n",
              "      <td>0.035020</td>\n",
              "      <td>0.03553</td>\n",
              "      <td>0.012260</td>\n",
              "      <td>0.02143</td>\n",
              "      <td>0.003749</td>\n",
              "      <td>15.49</td>\n",
              "      <td>30.73</td>\n",
              "      <td>106.20</td>\n",
              "      <td>739.3</td>\n",
              "      <td>0.1703</td>\n",
              "      <td>0.5401</td>\n",
              "      <td>0.5390</td>\n",
              "      <td>0.20600</td>\n",
              "      <td>0.4378</td>\n",
              "      <td>0.10720</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>84501001</td>\n",
              "      <td>M</td>\n",
              "      <td>12.46</td>\n",
              "      <td>24.04</td>\n",
              "      <td>83.97</td>\n",
              "      <td>475.9</td>\n",
              "      <td>0.11860</td>\n",
              "      <td>0.23960</td>\n",
              "      <td>0.22730</td>\n",
              "      <td>0.08543</td>\n",
              "      <td>0.2030</td>\n",
              "      <td>0.08243</td>\n",
              "      <td>0.2976</td>\n",
              "      <td>1.5990</td>\n",
              "      <td>2.039</td>\n",
              "      <td>23.94</td>\n",
              "      <td>0.007149</td>\n",
              "      <td>0.072170</td>\n",
              "      <td>0.07743</td>\n",
              "      <td>0.014320</td>\n",
              "      <td>0.01789</td>\n",
              "      <td>0.010080</td>\n",
              "      <td>15.09</td>\n",
              "      <td>40.68</td>\n",
              "      <td>97.65</td>\n",
              "      <td>711.4</td>\n",
              "      <td>0.1853</td>\n",
              "      <td>1.0580</td>\n",
              "      <td>1.1050</td>\n",
              "      <td>0.22100</td>\n",
              "      <td>0.4366</td>\n",
              "      <td>0.20750</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>845636</td>\n",
              "      <td>M</td>\n",
              "      <td>16.02</td>\n",
              "      <td>23.24</td>\n",
              "      <td>102.70</td>\n",
              "      <td>797.8</td>\n",
              "      <td>0.08206</td>\n",
              "      <td>0.06669</td>\n",
              "      <td>0.03299</td>\n",
              "      <td>0.03323</td>\n",
              "      <td>0.1528</td>\n",
              "      <td>0.05697</td>\n",
              "      <td>0.3795</td>\n",
              "      <td>1.1870</td>\n",
              "      <td>2.466</td>\n",
              "      <td>40.51</td>\n",
              "      <td>0.004029</td>\n",
              "      <td>0.009269</td>\n",
              "      <td>0.01101</td>\n",
              "      <td>0.007591</td>\n",
              "      <td>0.01460</td>\n",
              "      <td>0.003042</td>\n",
              "      <td>19.19</td>\n",
              "      <td>33.88</td>\n",
              "      <td>123.80</td>\n",
              "      <td>1150.0</td>\n",
              "      <td>0.1181</td>\n",
              "      <td>0.1551</td>\n",
              "      <td>0.1459</td>\n",
              "      <td>0.09975</td>\n",
              "      <td>0.2948</td>\n",
              "      <td>0.08452</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>84610002</td>\n",
              "      <td>M</td>\n",
              "      <td>15.78</td>\n",
              "      <td>17.89</td>\n",
              "      <td>103.60</td>\n",
              "      <td>781.0</td>\n",
              "      <td>0.09710</td>\n",
              "      <td>0.12920</td>\n",
              "      <td>0.09954</td>\n",
              "      <td>0.06606</td>\n",
              "      <td>0.1842</td>\n",
              "      <td>0.06082</td>\n",
              "      <td>0.5058</td>\n",
              "      <td>0.9849</td>\n",
              "      <td>3.564</td>\n",
              "      <td>54.16</td>\n",
              "      <td>0.005771</td>\n",
              "      <td>0.040610</td>\n",
              "      <td>0.02791</td>\n",
              "      <td>0.012820</td>\n",
              "      <td>0.02008</td>\n",
              "      <td>0.004144</td>\n",
              "      <td>20.42</td>\n",
              "      <td>27.28</td>\n",
              "      <td>136.50</td>\n",
              "      <td>1299.0</td>\n",
              "      <td>0.1396</td>\n",
              "      <td>0.5609</td>\n",
              "      <td>0.3965</td>\n",
              "      <td>0.18100</td>\n",
              "      <td>0.3792</td>\n",
              "      <td>0.10480</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>846226</td>\n",
              "      <td>M</td>\n",
              "      <td>19.17</td>\n",
              "      <td>24.80</td>\n",
              "      <td>132.40</td>\n",
              "      <td>1123.0</td>\n",
              "      <td>0.09740</td>\n",
              "      <td>0.24580</td>\n",
              "      <td>0.20650</td>\n",
              "      <td>0.11180</td>\n",
              "      <td>0.2397</td>\n",
              "      <td>0.07800</td>\n",
              "      <td>0.9555</td>\n",
              "      <td>3.5680</td>\n",
              "      <td>11.070</td>\n",
              "      <td>116.20</td>\n",
              "      <td>0.003139</td>\n",
              "      <td>0.082970</td>\n",
              "      <td>0.08890</td>\n",
              "      <td>0.040900</td>\n",
              "      <td>0.04484</td>\n",
              "      <td>0.012840</td>\n",
              "      <td>20.96</td>\n",
              "      <td>29.94</td>\n",
              "      <td>151.70</td>\n",
              "      <td>1332.0</td>\n",
              "      <td>0.1037</td>\n",
              "      <td>0.3903</td>\n",
              "      <td>0.3639</td>\n",
              "      <td>0.17670</td>\n",
              "      <td>0.3176</td>\n",
              "      <td>0.10230</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>846381</td>\n",
              "      <td>M</td>\n",
              "      <td>15.85</td>\n",
              "      <td>23.95</td>\n",
              "      <td>103.70</td>\n",
              "      <td>782.7</td>\n",
              "      <td>0.08401</td>\n",
              "      <td>0.10020</td>\n",
              "      <td>0.09938</td>\n",
              "      <td>0.05364</td>\n",
              "      <td>0.1847</td>\n",
              "      <td>0.05338</td>\n",
              "      <td>0.4033</td>\n",
              "      <td>1.0780</td>\n",
              "      <td>2.903</td>\n",
              "      <td>36.58</td>\n",
              "      <td>0.009769</td>\n",
              "      <td>0.031260</td>\n",
              "      <td>0.05051</td>\n",
              "      <td>0.019920</td>\n",
              "      <td>0.02981</td>\n",
              "      <td>0.003002</td>\n",
              "      <td>16.84</td>\n",
              "      <td>27.66</td>\n",
              "      <td>112.00</td>\n",
              "      <td>876.5</td>\n",
              "      <td>0.1131</td>\n",
              "      <td>0.1924</td>\n",
              "      <td>0.2322</td>\n",
              "      <td>0.11190</td>\n",
              "      <td>0.2809</td>\n",
              "      <td>0.06287</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>84667401</td>\n",
              "      <td>M</td>\n",
              "      <td>13.73</td>\n",
              "      <td>22.61</td>\n",
              "      <td>93.60</td>\n",
              "      <td>578.3</td>\n",
              "      <td>0.11310</td>\n",
              "      <td>0.22930</td>\n",
              "      <td>0.21280</td>\n",
              "      <td>0.08025</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.07682</td>\n",
              "      <td>0.2121</td>\n",
              "      <td>1.1690</td>\n",
              "      <td>2.061</td>\n",
              "      <td>19.21</td>\n",
              "      <td>0.006429</td>\n",
              "      <td>0.059360</td>\n",
              "      <td>0.05501</td>\n",
              "      <td>0.016280</td>\n",
              "      <td>0.01961</td>\n",
              "      <td>0.008093</td>\n",
              "      <td>15.03</td>\n",
              "      <td>32.01</td>\n",
              "      <td>108.80</td>\n",
              "      <td>697.7</td>\n",
              "      <td>0.1651</td>\n",
              "      <td>0.7725</td>\n",
              "      <td>0.6943</td>\n",
              "      <td>0.22080</td>\n",
              "      <td>0.3596</td>\n",
              "      <td>0.14310</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>84799002</td>\n",
              "      <td>M</td>\n",
              "      <td>14.54</td>\n",
              "      <td>27.54</td>\n",
              "      <td>96.73</td>\n",
              "      <td>658.8</td>\n",
              "      <td>0.11390</td>\n",
              "      <td>0.15950</td>\n",
              "      <td>0.16390</td>\n",
              "      <td>0.07364</td>\n",
              "      <td>0.2303</td>\n",
              "      <td>0.07077</td>\n",
              "      <td>0.3700</td>\n",
              "      <td>1.0330</td>\n",
              "      <td>2.879</td>\n",
              "      <td>32.55</td>\n",
              "      <td>0.005607</td>\n",
              "      <td>0.042400</td>\n",
              "      <td>0.04741</td>\n",
              "      <td>0.010900</td>\n",
              "      <td>0.01857</td>\n",
              "      <td>0.005466</td>\n",
              "      <td>17.46</td>\n",
              "      <td>37.13</td>\n",
              "      <td>124.10</td>\n",
              "      <td>943.2</td>\n",
              "      <td>0.1678</td>\n",
              "      <td>0.6577</td>\n",
              "      <td>0.7026</td>\n",
              "      <td>0.17120</td>\n",
              "      <td>0.4218</td>\n",
              "      <td>0.13410</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>848406</td>\n",
              "      <td>M</td>\n",
              "      <td>14.68</td>\n",
              "      <td>20.13</td>\n",
              "      <td>94.74</td>\n",
              "      <td>684.5</td>\n",
              "      <td>0.09867</td>\n",
              "      <td>0.07200</td>\n",
              "      <td>0.07395</td>\n",
              "      <td>0.05259</td>\n",
              "      <td>0.1586</td>\n",
              "      <td>0.05922</td>\n",
              "      <td>0.4727</td>\n",
              "      <td>1.2400</td>\n",
              "      <td>3.195</td>\n",
              "      <td>45.40</td>\n",
              "      <td>0.005718</td>\n",
              "      <td>0.011620</td>\n",
              "      <td>0.01998</td>\n",
              "      <td>0.011090</td>\n",
              "      <td>0.01410</td>\n",
              "      <td>0.002085</td>\n",
              "      <td>19.07</td>\n",
              "      <td>30.88</td>\n",
              "      <td>123.40</td>\n",
              "      <td>1138.0</td>\n",
              "      <td>0.1464</td>\n",
              "      <td>0.1871</td>\n",
              "      <td>0.2914</td>\n",
              "      <td>0.16090</td>\n",
              "      <td>0.3029</td>\n",
              "      <td>0.08216</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>84862001</td>\n",
              "      <td>M</td>\n",
              "      <td>16.13</td>\n",
              "      <td>20.68</td>\n",
              "      <td>108.10</td>\n",
              "      <td>798.8</td>\n",
              "      <td>0.11700</td>\n",
              "      <td>0.20220</td>\n",
              "      <td>0.17220</td>\n",
              "      <td>0.10280</td>\n",
              "      <td>0.2164</td>\n",
              "      <td>0.07356</td>\n",
              "      <td>0.5692</td>\n",
              "      <td>1.0730</td>\n",
              "      <td>3.854</td>\n",
              "      <td>54.18</td>\n",
              "      <td>0.007026</td>\n",
              "      <td>0.025010</td>\n",
              "      <td>0.03188</td>\n",
              "      <td>0.012970</td>\n",
              "      <td>0.01689</td>\n",
              "      <td>0.004142</td>\n",
              "      <td>20.96</td>\n",
              "      <td>31.48</td>\n",
              "      <td>136.80</td>\n",
              "      <td>1315.0</td>\n",
              "      <td>0.1789</td>\n",
              "      <td>0.4233</td>\n",
              "      <td>0.4784</td>\n",
              "      <td>0.20730</td>\n",
              "      <td>0.3706</td>\n",
              "      <td>0.11420</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>849014</td>\n",
              "      <td>M</td>\n",
              "      <td>19.81</td>\n",
              "      <td>22.15</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1260.0</td>\n",
              "      <td>0.09831</td>\n",
              "      <td>0.10270</td>\n",
              "      <td>0.14790</td>\n",
              "      <td>0.09498</td>\n",
              "      <td>0.1582</td>\n",
              "      <td>0.05395</td>\n",
              "      <td>0.7582</td>\n",
              "      <td>1.0170</td>\n",
              "      <td>5.865</td>\n",
              "      <td>112.40</td>\n",
              "      <td>0.006494</td>\n",
              "      <td>0.018930</td>\n",
              "      <td>0.03391</td>\n",
              "      <td>0.015210</td>\n",
              "      <td>0.01356</td>\n",
              "      <td>0.001997</td>\n",
              "      <td>27.32</td>\n",
              "      <td>30.88</td>\n",
              "      <td>186.80</td>\n",
              "      <td>2398.0</td>\n",
              "      <td>0.1512</td>\n",
              "      <td>0.3150</td>\n",
              "      <td>0.5372</td>\n",
              "      <td>0.23880</td>\n",
              "      <td>0.2768</td>\n",
              "      <td>0.07615</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>8510426</td>\n",
              "      <td>B</td>\n",
              "      <td>13.54</td>\n",
              "      <td>14.36</td>\n",
              "      <td>87.46</td>\n",
              "      <td>566.3</td>\n",
              "      <td>0.09779</td>\n",
              "      <td>0.08129</td>\n",
              "      <td>0.06664</td>\n",
              "      <td>0.04781</td>\n",
              "      <td>0.1885</td>\n",
              "      <td>0.05766</td>\n",
              "      <td>0.2699</td>\n",
              "      <td>0.7886</td>\n",
              "      <td>2.058</td>\n",
              "      <td>23.56</td>\n",
              "      <td>0.008462</td>\n",
              "      <td>0.014600</td>\n",
              "      <td>0.02387</td>\n",
              "      <td>0.013150</td>\n",
              "      <td>0.01980</td>\n",
              "      <td>0.002300</td>\n",
              "      <td>15.11</td>\n",
              "      <td>19.26</td>\n",
              "      <td>99.70</td>\n",
              "      <td>711.2</td>\n",
              "      <td>0.1440</td>\n",
              "      <td>0.1773</td>\n",
              "      <td>0.2390</td>\n",
              "      <td>0.12880</td>\n",
              "      <td>0.2977</td>\n",
              "      <td>0.07259</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id diagnosis  ...  fractal_dimension_worst  Unnamed: 32\n",
              "0     842302         M  ...                  0.11890          NaN\n",
              "1     842517         M  ...                  0.08902          NaN\n",
              "2   84300903         M  ...                  0.08758          NaN\n",
              "3   84348301         M  ...                  0.17300          NaN\n",
              "4   84358402         M  ...                  0.07678          NaN\n",
              "5     843786         M  ...                  0.12440          NaN\n",
              "6     844359         M  ...                  0.08368          NaN\n",
              "7   84458202         M  ...                  0.11510          NaN\n",
              "8     844981         M  ...                  0.10720          NaN\n",
              "9   84501001         M  ...                  0.20750          NaN\n",
              "10    845636         M  ...                  0.08452          NaN\n",
              "11  84610002         M  ...                  0.10480          NaN\n",
              "12    846226         M  ...                  0.10230          NaN\n",
              "13    846381         M  ...                  0.06287          NaN\n",
              "14  84667401         M  ...                  0.14310          NaN\n",
              "15  84799002         M  ...                  0.13410          NaN\n",
              "16    848406         M  ...                  0.08216          NaN\n",
              "17  84862001         M  ...                  0.11420          NaN\n",
              "18    849014         M  ...                  0.07615          NaN\n",
              "19   8510426         B  ...                  0.07259          NaN\n",
              "\n",
              "[20 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UheCi8cf48Un"
      },
      "source": [
        "Let's have a look at the health of the data itself. This function `.info()` in the pandas library is very helpful to understand the basic properties of the data itself. If there is any missing values in the dataset can be known right from here so that they can be taken care of before fitting into a model for training and testing. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UL33cILW48Uo",
        "outputId": "7ceb167d-2c17-42a9-a8a9-869cdf581064",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        }
      },
      "source": [
        "data.info()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 569 entries, 0 to 568\n",
            "Data columns (total 33 columns):\n",
            " #   Column                   Non-Null Count  Dtype  \n",
            "---  ------                   --------------  -----  \n",
            " 0   id                       569 non-null    int64  \n",
            " 1   diagnosis                569 non-null    object \n",
            " 2   radius_mean              569 non-null    float64\n",
            " 3   texture_mean             569 non-null    float64\n",
            " 4   perimeter_mean           569 non-null    float64\n",
            " 5   area_mean                569 non-null    float64\n",
            " 6   smoothness_mean          569 non-null    float64\n",
            " 7   compactness_mean         569 non-null    float64\n",
            " 8   concavity_mean           569 non-null    float64\n",
            " 9   concave points_mean      569 non-null    float64\n",
            " 10  symmetry_mean            569 non-null    float64\n",
            " 11  fractal_dimension_mean   569 non-null    float64\n",
            " 12  radius_se                569 non-null    float64\n",
            " 13  texture_se               569 non-null    float64\n",
            " 14  perimeter_se             569 non-null    float64\n",
            " 15  area_se                  569 non-null    float64\n",
            " 16  smoothness_se            569 non-null    float64\n",
            " 17  compactness_se           569 non-null    float64\n",
            " 18  concavity_se             569 non-null    float64\n",
            " 19  concave points_se        569 non-null    float64\n",
            " 20  symmetry_se              569 non-null    float64\n",
            " 21  fractal_dimension_se     569 non-null    float64\n",
            " 22  radius_worst             569 non-null    float64\n",
            " 23  texture_worst            569 non-null    float64\n",
            " 24  perimeter_worst          569 non-null    float64\n",
            " 25  area_worst               569 non-null    float64\n",
            " 26  smoothness_worst         569 non-null    float64\n",
            " 27  compactness_worst        569 non-null    float64\n",
            " 28  concavity_worst          569 non-null    float64\n",
            " 29  concave points_worst     569 non-null    float64\n",
            " 30  symmetry_worst           569 non-null    float64\n",
            " 31  fractal_dimension_worst  569 non-null    float64\n",
            " 32  Unnamed: 32              0 non-null      float64\n",
            "dtypes: float64(31), int64(1), object(1)\n",
            "memory usage: 146.8+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSh35FCx48Us"
      },
      "source": [
        "**There are 4 things that take my attention**\n",
        "\n",
        "1. There is an **`id`** that cannot be used for classificaiton \n",
        "2. **`Diagnosis`** column in the data is our class label\n",
        "3. **`Unnamed: 32`** feature includes **`NaN`** so we do not need it.\n",
        "4. I do not have any idea about other feature names actually.\n",
        "\n",
        "Therefore, drop these unnecessary features. However do not forget this is not a feature selection. This is like a browse a pub, we do not choose our drink yet !!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "r949QDzW48Us",
        "outputId": "9c4c7337-e8db-44a3-f7af-a484a805d1b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        }
      },
      "source": [
        "# feature names as a list\n",
        "# .columns gives columns names in data \n",
        "col = data.columns       \n",
        "print(col)\n",
        "\n",
        "\n",
        "data.drop(['Unnamed: 32',\"id\"], axis=1, inplace=True)\n",
        "data.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]\n",
        "y_data = data.diagnosis.values\n",
        "x_data = data.drop(['diagnosis'], axis=1)\n",
        "x_data"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
            "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
            "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
            "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
            "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
            "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
            "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
            "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
            "       'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.380</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.16220</td>\n",
              "      <td>0.66560</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.990</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.12380</td>\n",
              "      <td>0.18660</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.570</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.14440</td>\n",
              "      <td>0.42450</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.910</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.20980</td>\n",
              "      <td>0.86630</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.540</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.13740</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>21.56</td>\n",
              "      <td>22.39</td>\n",
              "      <td>142.00</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>0.11100</td>\n",
              "      <td>0.11590</td>\n",
              "      <td>0.24390</td>\n",
              "      <td>0.13890</td>\n",
              "      <td>0.1726</td>\n",
              "      <td>0.05623</td>\n",
              "      <td>1.1760</td>\n",
              "      <td>1.2560</td>\n",
              "      <td>7.673</td>\n",
              "      <td>158.70</td>\n",
              "      <td>0.010300</td>\n",
              "      <td>0.02891</td>\n",
              "      <td>0.05198</td>\n",
              "      <td>0.02454</td>\n",
              "      <td>0.01114</td>\n",
              "      <td>0.004239</td>\n",
              "      <td>25.450</td>\n",
              "      <td>26.40</td>\n",
              "      <td>166.10</td>\n",
              "      <td>2027.0</td>\n",
              "      <td>0.14100</td>\n",
              "      <td>0.21130</td>\n",
              "      <td>0.4107</td>\n",
              "      <td>0.2216</td>\n",
              "      <td>0.2060</td>\n",
              "      <td>0.07115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>20.13</td>\n",
              "      <td>28.25</td>\n",
              "      <td>131.20</td>\n",
              "      <td>1261.0</td>\n",
              "      <td>0.09780</td>\n",
              "      <td>0.10340</td>\n",
              "      <td>0.14400</td>\n",
              "      <td>0.09791</td>\n",
              "      <td>0.1752</td>\n",
              "      <td>0.05533</td>\n",
              "      <td>0.7655</td>\n",
              "      <td>2.4630</td>\n",
              "      <td>5.203</td>\n",
              "      <td>99.04</td>\n",
              "      <td>0.005769</td>\n",
              "      <td>0.02423</td>\n",
              "      <td>0.03950</td>\n",
              "      <td>0.01678</td>\n",
              "      <td>0.01898</td>\n",
              "      <td>0.002498</td>\n",
              "      <td>23.690</td>\n",
              "      <td>38.25</td>\n",
              "      <td>155.00</td>\n",
              "      <td>1731.0</td>\n",
              "      <td>0.11660</td>\n",
              "      <td>0.19220</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.2572</td>\n",
              "      <td>0.06637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>16.60</td>\n",
              "      <td>28.08</td>\n",
              "      <td>108.30</td>\n",
              "      <td>858.1</td>\n",
              "      <td>0.08455</td>\n",
              "      <td>0.10230</td>\n",
              "      <td>0.09251</td>\n",
              "      <td>0.05302</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>0.05648</td>\n",
              "      <td>0.4564</td>\n",
              "      <td>1.0750</td>\n",
              "      <td>3.425</td>\n",
              "      <td>48.55</td>\n",
              "      <td>0.005903</td>\n",
              "      <td>0.03731</td>\n",
              "      <td>0.04730</td>\n",
              "      <td>0.01557</td>\n",
              "      <td>0.01318</td>\n",
              "      <td>0.003892</td>\n",
              "      <td>18.980</td>\n",
              "      <td>34.12</td>\n",
              "      <td>126.70</td>\n",
              "      <td>1124.0</td>\n",
              "      <td>0.11390</td>\n",
              "      <td>0.30940</td>\n",
              "      <td>0.3403</td>\n",
              "      <td>0.1418</td>\n",
              "      <td>0.2218</td>\n",
              "      <td>0.07820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>20.60</td>\n",
              "      <td>29.33</td>\n",
              "      <td>140.10</td>\n",
              "      <td>1265.0</td>\n",
              "      <td>0.11780</td>\n",
              "      <td>0.27700</td>\n",
              "      <td>0.35140</td>\n",
              "      <td>0.15200</td>\n",
              "      <td>0.2397</td>\n",
              "      <td>0.07016</td>\n",
              "      <td>0.7260</td>\n",
              "      <td>1.5950</td>\n",
              "      <td>5.772</td>\n",
              "      <td>86.22</td>\n",
              "      <td>0.006522</td>\n",
              "      <td>0.06158</td>\n",
              "      <td>0.07117</td>\n",
              "      <td>0.01664</td>\n",
              "      <td>0.02324</td>\n",
              "      <td>0.006185</td>\n",
              "      <td>25.740</td>\n",
              "      <td>39.42</td>\n",
              "      <td>184.60</td>\n",
              "      <td>1821.0</td>\n",
              "      <td>0.16500</td>\n",
              "      <td>0.86810</td>\n",
              "      <td>0.9387</td>\n",
              "      <td>0.2650</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.12400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>7.76</td>\n",
              "      <td>24.54</td>\n",
              "      <td>47.92</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.05263</td>\n",
              "      <td>0.04362</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.1587</td>\n",
              "      <td>0.05884</td>\n",
              "      <td>0.3857</td>\n",
              "      <td>1.4280</td>\n",
              "      <td>2.548</td>\n",
              "      <td>19.15</td>\n",
              "      <td>0.007189</td>\n",
              "      <td>0.00466</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.02676</td>\n",
              "      <td>0.002783</td>\n",
              "      <td>9.456</td>\n",
              "      <td>30.37</td>\n",
              "      <td>59.16</td>\n",
              "      <td>268.6</td>\n",
              "      <td>0.08996</td>\n",
              "      <td>0.06444</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2871</td>\n",
              "      <td>0.07039</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>569 rows × 30 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     radius_mean  texture_mean  ...  symmetry_worst  fractal_dimension_worst\n",
              "0          17.99         10.38  ...          0.4601                  0.11890\n",
              "1          20.57         17.77  ...          0.2750                  0.08902\n",
              "2          19.69         21.25  ...          0.3613                  0.08758\n",
              "3          11.42         20.38  ...          0.6638                  0.17300\n",
              "4          20.29         14.34  ...          0.2364                  0.07678\n",
              "..           ...           ...  ...             ...                      ...\n",
              "564        21.56         22.39  ...          0.2060                  0.07115\n",
              "565        20.13         28.25  ...          0.2572                  0.06637\n",
              "566        16.60         28.08  ...          0.2218                  0.07820\n",
              "567        20.60         29.33  ...          0.4087                  0.12400\n",
              "568         7.76         24.54  ...          0.2871                  0.07039\n",
              "\n",
              "[569 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZmPQ3qi48Uw"
      },
      "source": [
        "Okey, now we have features but **`what does they mean`** or actually **`how much do we need to know about these features`**\n",
        "The answer is that we do not need to know meaning of these features however in order to imagine in our mind we should know something like variance, standart deviation, number of sample (count) or max min values.\n",
        "These type of information helps to understand about what is going on data. For example , the question is appeared in my mind the **`area_mean`** feature's max value is 2500 and **`smoothness_mean`** features' max 0.16340. Therefore **do we need standirdization or normalization before visualization, feature selection, feature extraction or classificaiton?** The answer is yes and no not surprising ha :) Anyway lets go step by step and start with visualization. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikmfvPcL48Ux"
      },
      "source": [
        "## Normalization\n",
        "Normalization refers to rescaling real-valued numeric attributes into a 0 to 1 range. Data normalization is used in machine learning to make model training less sensitive to the scale of features.\n",
        "\n",
        "You can either implement the conversion process with basic python or use `MinMaxScaler()` function from the `sklearn` library. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BZfZTspj48Ux"
      },
      "source": [
        "# Using transformer from sklearn library\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scalar = MinMaxScaler()\n",
        "output = scalar.fit_transform(x_data)\n",
        "\n",
        "# Manual Implementation of the normalization process\n",
        "X_data = (x_data -np.min(x_data))/ (np.max(x_data)-np.min(x_data)).values"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faBBeeIw48U7"
      },
      "source": [
        "## The Basics: Logistic Regression and Regularization\n",
        "\n",
        "Logistic Regression is one of the most common machine learning algorithms used for classification. It a statistical model that uses a logistic function to model a binary dependent variable. In essence, it predicts the probability of an observation belonging to a certain class or label. For instance, is this a cat photo or a dog photo?\n",
        "\n",
        "Ordinary Least Squares linear regression is powerful and versatile right out of the box, but there are certain circumstances where it fails. \n",
        "1. it is, expressly, a ‘regression’ framework, which makes it hard to apply as a classifier.\n",
        "2. unlike, say, a decision tree, linear regression models don’t perform their own implicit feature selection, meaning they are prone to overfit if too many features are included. \n",
        "\n",
        "Luckily, there are some extensions to the linear model that allow us to overcome these issues. Logistic regression turns the linear regression framework into a classifier and various types of **`regularization`** of which the `Ridge` and `Lasso` methods are most common, help avoid overfit in feature rich instances.\n",
        "\n",
        "### **Hypothesis:** \n",
        "We want our model to predict the probability of an observation belonging to a certain class or label. As such, we want a hypothesis $h$ that satisfies the following condition $0 <= h(x) <= 1$ , where $x$ is an observation.\n",
        "\n",
        "We define $h(x) = g(w^T * x)$ , where $g$ is a sigmoid function and $w$ are the trainable parameters or `weights`. As such, we have:\n",
        "$$h(x) = \\frac{1}{1+e^{-w^Tx}}$$\n",
        "\n",
        "### The cost for an observation: \n",
        "Now that we can predict the probability for an observation, we want the result to have the minimum error. If the class label is $y$, the cost (error) associated with an observation $x$ is given by:\n",
        "\n",
        "![](https://miro.medium.com/max/525/1*vSGnYVz6I7sAObKuxuFAoQ.gif)\n",
        "\n",
        "### Cost Function: \n",
        "Thus, the total cost for all the $m$ observations in a dataset is:\n",
        "![](https://miro.medium.com/max/368/0*vZnp94vCoN0vMDAj)\n",
        "\n",
        "We can rewrite the cost function J as:\n",
        "![](https://miro.medium.com/max/691/0*o57ug0iMGDJVI1qo)\n",
        "\n",
        "The objective of logistic regression is to find params `w` so that `J` is minimum. How can we do that?? We will use the gradient descent algorithm to update each of the weights gradually to minimize the cost `J`. \n",
        "\n",
        "We will update each of the params wᵢ using the following template:\n",
        "![](https://miro.medium.com/max/875/0*Q6ssvXABrvHUZrfy)\n",
        "![](https://miro.medium.com/max/496/0*7uVvuW-ZGauNWH_V)\n",
        "\n",
        "The above step will help us find a set of params wᵢ, which will then help us to come up with $h(x)$ to solve our binary classification task.\n",
        "But there is also an undesirable outcome associated with the above gradient descent steps. In an attempt to find the best $h(x)$, the following things happen:\n",
        "\n",
        "**CASE I: For class label = 0**: $h(x)$ will try to produce results as close 0 as possible. As such, $w^T.x$ will be as small as possible\n",
        "=> Wi will tend to -infinity\n",
        "\n",
        "**CASE II: For class label = 1**: $h(x)$ will try to produce results as close 1 as possible. As such, $w^T.x$ will be as large as possible\n",
        "=> Wi will tend to +infinity\n",
        "\n",
        "\n",
        "## Regularization:\n",
        "Regularization is a technique to solve the problem of overfitting in a machine learning algorithm by penalizing the cost function. It does so by using an additional penalty term in the cost function.\n",
        "There are two types of regularization techniques:\n",
        "1. Lasso or L1 Regularization\n",
        "2. Ridge or L2 Regularization (We will implement here)\n",
        "So, how can L2 Regularization help to prevent overfitting? Let’s first look at our new cost function:\n",
        "\n",
        "![](https://miro.medium.com/max/628/0*Nc_ocecF0dHpUutK)\n",
        "\n",
        "\n",
        "The regularization term will heavily penalize large $w_i$. The effect will be less on smaller $w_i$’s. As such, the growth of $w$ is controlled. The $h(x)$ we obtain with these controlled params $w$ will be more generalizable.\n",
        "\n",
        "**NOTE:** $λ$ is a hyper-parameter value. We have to find it using cross-validation. \n",
        "* Larger value $λ$ of will make $w_i$ shrink closer to $0$, which might lead to underfitting. \n",
        "* $λ = 0$, will have no regulariztion effect. \n",
        "\n",
        "When choosing $λ$, we have to take proper care of bias vs variance trade-off.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "SIKhL69648U8"
      },
      "source": [
        "class LogisticRegression(object):\n",
        "    \"\"\"\n",
        "    Logistic Regression Classifier\n",
        "    Parameters\n",
        "    ----------\n",
        "    learning_rate : int or float, default=0.1\n",
        "        The tuning parameter for the optimization algorithm (here, Gradient Descent) \n",
        "        that determines the step size at each iteration while moving toward a minimum \n",
        "        of the cost function.\n",
        "    max_iter : int, default=100\n",
        "        Maximum number of iterations taken for the optimization algorithm to converge\n",
        "    \n",
        "    penalty : None or 'l2', default='l2'.\n",
        "        Option to perform L2 regularization.\n",
        "    C : float, default=0.1\n",
        "        Inverse of regularization strength; must be a positive float. \n",
        "        Smaller values specify stronger regularization. \n",
        "    tolerance : float, optional, default=1e-4\n",
        "        Value indicating the weight change between epochs in which\n",
        "        gradient descent should terminated. \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, learning_rate=0.1, max_iter=100, regularization='l2', lambda_ = 10 , tolerance = 1e-4):\n",
        "        self.learning_rate  = learning_rate\n",
        "        self.max_iter       = max_iter\n",
        "        self.regularization = regularization\n",
        "        self.lambda_        = lambda_\n",
        "        self.tolerance      = tolerance\n",
        "        self.loss_log       = []\n",
        "    \n",
        "    def fit(self, X, y, verbose = False):\n",
        "        \"\"\"\n",
        "        Fit the model according to the given training data.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
        "            Training vector, where n_samples is the number of samples and\n",
        "            n_features is the number of features.\n",
        "        y : array-like of shape (n_samples,)\n",
        "            Target vector relative to X.\n",
        "        Returns\n",
        "        -------\n",
        "        self : object\n",
        "        \"\"\"\n",
        "        self.theta = np.random.rand(X.shape[1] + 1)\n",
        "        X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
        "        self.loss_log = []\n",
        "\n",
        "        for iteration in range(self.max_iter):\n",
        "            Z = np.matmul(X,  self.theta)\n",
        "            y_hat = self.__sigmoid(Z)\n",
        "            \n",
        "            errors = y_hat - y\n",
        "\n",
        "            N = X.shape[1] \n",
        "            \n",
        "            cost = (-1.0/N) * np.sum( y*np.log(y_hat) + (1.0 - y)*np.log(1.0-y_hat))\n",
        "            self.loss_log.append(cost)\n",
        "            \n",
        "            if verbose:\n",
        "                print(f'Iteration {iteration} Loss: {cost}')\n",
        "\n",
        "            if self.regularization is not None:\n",
        "                delta_grad = (1./N) *(np.matmul(errors.T, X)+ self.lambda_ * self.theta)\n",
        "            else:\n",
        "                delta_grad = (1./N) *(np.matmul(errors.T, X))\n",
        "                \n",
        "            self.theta -= self.learning_rate * delta_grad\n",
        "\n",
        "#             if np.all(abs(delta_grad) >= self.tolerance):\n",
        "#                 self.theta -= self.learning_rate * delta_grad\n",
        "#             else:\n",
        "#                 break\n",
        "                \n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"\n",
        "        Probability estimates for samples in X.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array-like of shape (n_samples, n_features)\n",
        "            Vector to be scored, where `n_samples` is the number of samples and\n",
        "            `n_features` is the number of features.\n",
        "        Returns\n",
        "        -------\n",
        "        probs : array-like of shape (n_samples,)\n",
        "            Returns the probability of each sample.\n",
        "        \"\"\"\n",
        "        return self.__sigmoid((X @ self.theta[1:]) + self.theta[0])\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict class labels for samples in X.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : array_like or sparse matrix, shape (n_samples, n_features)\n",
        "            Samples.\n",
        "        Returns\n",
        "        -------\n",
        "        labels : array, shape [n_samples]\n",
        "            Predicted class label per sample.\n",
        "        \"\"\"\n",
        "        return np.round(self.predict_proba(X))\n",
        "        \n",
        "    def __sigmoid(self, z):\n",
        "        \"\"\"\n",
        "        The sigmoid function.\n",
        "        Parameters\n",
        "        ------------\n",
        "        z : float\n",
        "            linear combinations of weights and sample features\n",
        "            z = w_0 + w_1*x_1 + ... + w_n*x_n\n",
        "        Returns\n",
        "        ---------\n",
        "        Value of logistic function at z\n",
        "        \"\"\"\n",
        "        return (1.0 / (1.0 + np.exp(-z)))\n",
        "\n",
        "    def get_params(self):\n",
        "        \"\"\"\n",
        "        Get method for models coeffients and intercept.\n",
        "        Returns\n",
        "        -------\n",
        "        params : dict\n",
        "        \"\"\"\n",
        "        try:\n",
        "            params = dict()\n",
        "            params['intercept'] = self.theta[0]\n",
        "            params['coef'] = self.theta[1:]\n",
        "            return params\n",
        "        except:\n",
        "            raise Exception('Fit the model first!')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eseYei7-48U_"
      },
      "source": [
        "### Train and Test Data Validation\n",
        "Let us split the whole data into two portion. We take 80% data in the train set and then put rest of the data into the test set to check the performance of the trained model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "w1HSZLIW48VA",
        "outputId": "d67405cf-ac50-4733-e69f-70eaea7a2b03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.20, random_state=42)\n",
        "\n",
        "# Train and Test Data Summary\n",
        "import plotly.graph_objects as go\n",
        "split = ['Train','Test']\n",
        "\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Bar(x=split, y=[np.sum(y_train), np.sum(y_test)],#                base=[-500,-600],\n",
        "                    marker_color='crimson',\n",
        "                    name='Malignant'))\n",
        "fig.add_trace(go.Bar(x=split, \n",
        "                     y=[len(y_train)- np.sum(y_train), len(y_test) - np.sum(y_test)],\n",
        "                    base=0,\n",
        "                    marker_color='lightgreen',\n",
        "                    name='Benign'                ))\n",
        "fig.update_layout(width = 800, height = 400)\n",
        "fig.update_layout(title = 'Count of Samples in Train and Test Split', title_x = 0.5, xaxis_title = \"Category\", yaxis_title = 'Sample Count')\n",
        "fig.show()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"8bcdc919-3136-4b73-994c-0d2265a66c1c\" class=\"plotly-graph-div\" style=\"height:400px; width:800px;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"8bcdc919-3136-4b73-994c-0d2265a66c1c\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '8bcdc919-3136-4b73-994c-0d2265a66c1c',\n",
              "                        [{\"marker\": {\"color\": \"crimson\"}, \"name\": \"Malignant\", \"type\": \"bar\", \"x\": [\"Train\", \"Test\"], \"y\": [169, 43]}, {\"base\": 0, \"marker\": {\"color\": \"lightgreen\"}, \"name\": \"Benign\", \"type\": \"bar\", \"x\": [\"Train\", \"Test\"], \"y\": [286, 71]}],\n",
              "                        {\"height\": 400, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Count of Samples in Train and Test Split\", \"x\": 0.5}, \"width\": 800, \"xaxis\": {\"title\": {\"text\": \"Category\"}}, \"yaxis\": {\"title\": {\"text\": \"Sample Count\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('8bcdc919-3136-4b73-994c-0d2265a66c1c');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQpWR7UrBv3L"
      },
      "source": [
        "## Training Logistic Regression\n",
        "Now we train the logistic regression with the training data for a maximum interation of 200. Other parameters are kept default. Feel free to fiddle around the other parameters to understand more of them. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "q0egCbIh48VD",
        "outputId": "c0aca880-27df-4370-de12-5dd849fb9f59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "clf = LogisticRegression(max_iter = 200, regularization= None)\n",
        "clf.fit(X_train, y_train, verbose = False)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.LogisticRegression at 0x7f9753a649b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9u52S-aCJ-w"
      },
      "source": [
        "### Plot Training Loss over Time "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkK3DdF2CJfi",
        "outputId": "b9969bed-21c8-44b7-f83d-d7c7eb4ebb39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        }
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "import numpy as np\n",
        "\n",
        "y = clf.loss_log \n",
        "\n",
        "fig = go.Figure(data=go.Scatter(x= np.arange(start =1, stop = len(y)), \n",
        "                                y=y,\n",
        "                                mode = 'lines+markers'))\n",
        "\n",
        "fig.update_layout(title = \"Error Plot over Iterations\", title_x = 0.5,\n",
        "                  xaxis_title = 'Iteration',\n",
        "                  yaxis_title = 'Log Loss',\n",
        "                  width = 800,\n",
        "                  height = 500)\n",
        "fig.show()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"25a1c383-6dbc-409c-948c-329ea0709803\" class=\"plotly-graph-div\" style=\"height:500px; width:800px;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"25a1c383-6dbc-409c-948c-329ea0709803\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '25a1c383-6dbc-409c-948c-329ea0709803',\n",
              "                        [{\"mode\": \"lines+markers\", \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199], \"y\": [26.24983749331236, 12.109968922122624, 8.1315660325315, 7.7951508994241285, 7.505337022899398, 7.243243518547837, 7.005164186669494, 6.7880408663704435, 6.5892782533896765, 6.4066683323319, 6.238326913787763, 6.082640500147864, 5.938222159266428, 5.803875025395285, 5.678562151152725, 5.561381596022452, 5.451545809270271, 5.348364526116186, 5.251230536845575, 5.159607807424082, 5.0730215284018625, 4.991049749008647, 4.913316318197992, 4.839484906687806, 4.769253926125105, 4.702352195361927, 4.638535231097656, 4.577582062140684, 4.519292484327787, 4.463484687557112, 4.409993198111578, 4.358667089007338, 4.309368418920476, 4.261970866661536, 4.216358533450208, 4.1724248896060345, 4.130071845886925, 4.0892089327130074, 4.049752573020052, 4.011625436583271, 3.9747558654114745, 3.93907736129175, 3.9045281278140647, 3.871050660262312, 3.838591377655451, 3.807100291985755, 3.776530710352369, 3.7468389662453747, 3.717984176713096, 3.689928022555824, 3.662634549042715, 3.636069984953944, 3.610202578014387, 3.585002445014248, 3.560441435111199, 3.53649300498201, 3.5131321046430077, 3.4903350728909963, 3.4680795414322003, 3.446344346868513, 3.4251094497997574, 3.4043558603794435, 3.384065569731013, 3.3642214866929496, 3.34480737941552, 3.3258078213801108, 3.307208141454887, 3.288994377638622, 3.271153234178373, 3.2536720417769693, 3.236538720633263, 3.2197417460822186, 3.203270116623603, 3.18711332414739, 3.171261326181425, 3.1557045200025695, 3.140433718466631, 3.125440127425083, 3.1107153246080608, 3.096251239863464, 3.082040136651358, 3.0680745947013723, 3.0543474937484403, 3.0408519982692312, 3.027581543147947, 3.0145298202059094, 3.0016907655346112, 2.98905854757668, 2.9766275559035487, 2.964392390642602, 2.952347852510206, 2.9404889334103355, 2.9288108075615527, 2.9173088231178803, 2.905978494251649, 2.8948154936687556, 2.8838156455288892, 2.872974918745301, 2.862289420640451, 2.851755390935597, 2.841369196053873, 2.831127323717868, 2.8210263778240097, 2.8110630735772406, 2.801234232870641, 2.7915367798956376, 2.781967736969419, 2.772524220567046, 2.7632034375465824, 2.754002681556307, 2.74491932961379, 2.7359508388472578, 2.72709474339029, 2.718348651421434, 2.709710242340867, 2.7011772640767227, 2.6927475305141235, 2.6844189190404273, 2.6761893682005566, 2.6680568754566525, 2.6600194950466505, 2.652075335936684, 2.6442225598625186, 2.6364593794555016, 2.628784056448785, 2.6211948999597974, 2.61369026484519, 2.6062685501246903, 2.5989281974704967, 2.5916676897590194, 2.5844855496819927, 2.5773803384140894, 2.5703506543343675, 2.563395131799013, 2.556512439962968, 2.549701281648174, 2.5429603922562825, 2.536288538723775, 2.5296845185175862, 2.523147158669375, 2.5166753148467107, 2.5102678704595394, 2.503923735800344, 2.497641847216527, 2.491421166313604, 2.4852606791878626, 2.4791593956872124, 2.4731163486990257, 2.467130593463804, 2.46120120691358, 2.4553272870340233, 2.4495079522492347, 2.4437423408283063, 2.4380296103127383, 2.4323689369638553, 2.426759515229401, 2.421200557228547, 2.4156912922545533, 2.410230966294385, 2.4048188415646123, 2.3994541960629303, 2.3941363231347044, 2.3888645310539367, 2.383638142618104, 2.378456494756318, 2.3733189381503044, 2.3682248368677077, 2.3631735680072485, 2.3581645213552935, 2.353197099053395, 2.3482707152764046, 2.3433847959207594, 2.338538778302564, 2.3337321108651072, 2.3289642528954775, 2.3242346742499302, 2.3195428550877057, 2.3148882856129793, 2.3102704658246678, 2.305688905273796, 2.3011431228281682, 2.296632646444083, 2.2921570129448408, 2.2877157678058095, 2.283308464945829, 2.278934666524718, 2.2745939427466944, 2.2702858716694867, 2.266010039018959, 2.2617660380090596, 2.2575534691669006, 2.2533719401628254, 2.2492210656452594, 2.245100467080227, 2.2410097725953433, 2.2369486168281627, 2.2329166407787233, 2.228913491666163, 2.22493882278926, 2.2209922933907986]}],\n",
              "                        {\"height\": 500, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"text\": \"Error Plot over Iterations\", \"x\": 0.5}, \"width\": 800, \"xaxis\": {\"title\": {\"text\": \"Iteration\"}}, \"yaxis\": {\"title\": {\"text\": \"Log Loss\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('25a1c383-6dbc-409c-948c-329ea0709803');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50y75O6BDRAg"
      },
      "source": [
        "## Accuracy Calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "vG00BWel48VH",
        "outputId": "e0965b46-d72d-4a6a-e49b-e39ec374d0c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_pred = clf.predict(X_test)\n",
        "y_pred\n",
        "\n",
        "match = (y_pred == y_test)*1.0\n",
        "match.values\n",
        "\n",
        "\n",
        "accuracy = (np.sum(match)*100/ len(match))\n",
        "print(f'Accuracy {accuracy}%')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy 96.49122807017544%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj3YQZaHEmlX"
      },
      "source": [
        "## Plot Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EovXZHzEGyo",
        "outputId": "f6e3502a-37e7-45de-a364-931349de222b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[70  1]\n",
            " [ 3 40]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f974e138cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSM73lMREQs9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}